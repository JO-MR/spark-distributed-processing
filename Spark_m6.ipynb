{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08db3b6a-9c7f-4b17-810e-e8494956f20f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d01e43162b64c90ce0048e8a23f3b1b",
     "grade": false,
     "grade_id": "cell-f8987996be9f1238",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Accidentes de tráfico en Reino Unido entre 2010 y 2014 \n",
    "\n",
    "### Disponible en Kaggle en:\n",
    "https://www.kaggle.com/stefanoleone992/adm-project-road-accidents-in-uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fbc52c4-73a9-49b7-8473-131f5409d5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a6b4dc108ddf890c659e33701965428",
     "grade": false,
     "grade_id": "cell-f74d7bfd01811789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Variables y significado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3992d775-4517-40ab-b5c5-f41eaf395374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a5a5882319ae0a14393c8d534816a56",
     "grade": false,
     "grade_id": "cell-9cfb34982bd4eb04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* Accident_Index: Accident index\n",
    "* Latitude: Accident latitude\n",
    "* Longitude: Accident longitude\n",
    "* Region: Accident region\n",
    "* Urban_or_Rural_Area: Accident area (rural or urban)\n",
    "* X1st_Road_Class: Accident road class\n",
    "* Driver_IMD_Decile: Road IMD Decile\n",
    "* Speed_limit: Road speed limit\n",
    "* Road_Type: Road type\n",
    "* Road_Surface_Conditions: Road surface condition\n",
    "* Weather: Weather\n",
    "* High_Wind: High wind\n",
    "* Lights: Road lights\n",
    "* Datetime: Accident datetime\n",
    "* Year: Accident year\n",
    "* Season: Accident season\n",
    "* Month_of_Year: Accident month\n",
    "* Day_of_Month: Accident day of month\n",
    "* Day_of_Week: Accident day of week\n",
    "* Hour_of_Day: Accident hour of day\n",
    "* Number_of_Vehicles: Accident number of vehicles\n",
    "* Age_of_Driver: Driver age\n",
    "* Age_of_Vehicle: Vehicle age\n",
    "* Junction_Detail: Accident junction detail\n",
    "* Junction_Location: Accident junction location\n",
    "* X1st_Point_of_Impact: Vehicle first point of impact\n",
    "* Driver_Journey_Purpose: Driver journey purpose\n",
    "* Engine_CC: Vehicle engine power (in CC)\n",
    "* Propulsion_Code: Vehicle propulsion code\n",
    "* Vehicle_Make: Vehicle brand\n",
    "* Vehicle_Category: Vehicle brand category\n",
    "* Vehicle_Manoeuvre: Vehicle manoeuvre when accident happened\n",
    "* Accident_Severity: Accident severity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f68dd4e-4720-46cb-bd35-5ab864822bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Nombre completo del alumno:**  Jonás De Martín Rodríguez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186071aa-9895-4cdd-bdc2-115629280442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adf0d762edf43c8674e5bbc71e788601",
     "grade": false,
     "grade_id": "cell-b4f9c37a2b92d2e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# INSTRUCCIONES \n",
    "\n",
    "En cada celda debes responder a la pregunta formulada, asegurándote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del cálculo quede guardado exactamente en la variable que venía inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). \n",
    "\n",
    "**No olvides borrar la línea *raise NotImplementedError()* de cada celda cuando hayas completado la solución de esa celda y quieras probarla**.\n",
    "\n",
    "Después de cada celda evaluable verás una celda con código. Ejecútala (no modifiques su código) y te dirá si tu solución es correcta o no. Además de esas pruebas, se realizarán algunas más (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la solución correcta o no. Asegúrate de que, al menos, todas las celdas indican que el código es correcto antes de enviar el notebook terminado.\n",
    "\n",
    "**Nunca se debe redondear ninguna cantidad si no lo pide explícitamente el enunciado**\n",
    "\n",
    "### Cada solución debe escribirse obligatoriamente en la celda habilitada para ello. Cualquier celda adicional que se haya creado durante el desarrollo deberá ser eliminada.\n",
    "\n",
    "Si necesitas crear celdas auxiliares durante el desarrollo, puedes hacerlo pero debes asegurarte de borrarlas antes de entregar el notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9053ec92-d959-4082-9bcd-430c82f4094c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7764e6064699f591cd2896d2430528e",
     "grade": false,
     "grade_id": "cell-69ec0993eeaff3ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Sobre el dataset anterior (accidents_uk.csv) se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e736ed-bb68-48a1-8994-e957de07e56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Ejercicio 1 (1.5 puntos)** \n",
    "* Leerlo tratando de que Spark infiera el tipo de dato de cada columna.\n",
    "* Crear una columna llamada `Age_Category` renombrando los valores de la columna `Age_of_Driver` donde los valores 1 y 2 de la columna original sean etiquetados en la columna nueva como \"Adolescente\", los valores 3 y 4 como \"Joven\", los valores 5 y 6 como \"Adulto\", y los valores 7 y 8 como \"Anciano\".\n",
    "* Crear una columna llamada `hora` aplicando la función `F.hour` a la columna `\"Datetime\"` ya existente.\n",
    "* El resultado debe guardarse **cacheado** en la variable `accidentesDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc88526-bb53-43c5-bb05-0c0423e78b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importo el módulo de funciones de PySpark SQL, que contiene transformaciones como when, col, hour, count, etc.\n",
    "# Uso el alias 'F' para referirme a esas funciones de forma abreviada (por ejemplo: F.col(\"columna\")).\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configuración para acceder a Azure Data Lake Storage (ADLS Gen2)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.masterjmr.dfs.core.windows.net\",\n",
    "    \"<REDACTED_ACCESS_KEY>\"\n",
    ")\n",
    "\n",
    "# Ruta del archivo CSV en ADLS\n",
    "ruta_csv = \"abfss://datos@masterjmr.dfs.core.windows.net/accidents_uk.csv\"\n",
    "\n",
    "# Lectura del CSV desde ADLS, con inferencia de tipos\n",
    "accidentesDF = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(ruta_csv)\n",
    "\n",
    "    # Creo una nueva columna llamada 'Age_Category' clasificando la edad del conductor en 4 grupos.\n",
    "    .withColumn(\n",
    "        \"Age_Category\",\n",
    "        F.when(F.col(\"Age_of_Driver\").isin(1, 2), \"Adolescente\")\n",
    "         .when(F.col(\"Age_of_Driver\").isin(3, 4), \"Joven\")\n",
    "         .when(F.col(\"Age_of_Driver\").isin(5, 6), \"Adulto\")\n",
    "         .when(F.col(\"Age_of_Driver\").isin(7, 8), \"Anciano\")\n",
    "    )\n",
    "\n",
    "    # Extraigo la hora del accidente, desde la columna 'Datetime'.\n",
    "    .withColumn(\"hora\", F.hour(\"Datetime\"))\n",
    "\n",
    "    # Cacheo el DataFrame, dado que se reutilizará en ejercicios posteriores.\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb9cdef-024a-487c-be17-264a0cc4ac19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "214402d3a6c78ca57ae876cb84f7e722",
     "grade": true,
     "grade_id": "read_csv_test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "assert(accidentesDF.schema[1].dataType == DoubleType())\n",
    "assert(accidentesDF.count() == 251832)\n",
    "\n",
    "assert(dict(accidentesDF.dtypes)[\"Age_Category\"] == \"string\")\n",
    "collectedDF = accidentesDF.groupBy(\"Age_Category\").count().orderBy(\"count\").collect()\n",
    "assert((collectedDF[0][\"count\"] == 22533) & (collectedDF[0][\"Age_Category\"] == \"Anciano\"))\n",
    "assert((collectedDF[1][\"count\"] == 57174) & (collectedDF[1][\"Age_Category\"] == \"Adolescente\"))\n",
    "assert((collectedDF[2][\"count\"] == 67138) & (collectedDF[2][\"Age_Category\"] == \"Adulto\"))\n",
    "assert((collectedDF[3][\"count\"] == 104987) & (collectedDF[3][\"Age_Category\"] == \"Joven\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13613d0e-9430-4d8b-8451-2d1004af801d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ea5104d104e3d22b6d9d8a1eba3b7b7",
     "grade": false,
     "grade_id": "cell-b90f5b934eda250e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 2 (3 puntos)** \n",
    "\n",
    "Partiendo de `accidentesDF`, queremos pegar (**sin hacer JOIN sino usando agregaciones sobre ventanas**) a cada accidente la siguiente información:\n",
    "* Número de accidentes que ha habido *en ese mismo año con esa misma categoría de vehículo*, en una nueva columna `total_vehiculo_anio`.\n",
    "* Número de accidentes que ha habido *en ese mismo año con esa misma categoría de vehículo y con esa misma situación (Junction Location)*, en una nueva columna `total_vehiculo_causa_anio`.\n",
    "* Porcentaje (en tanto por uno) que supone el segundo dato sobre el primero, en la columna `porc_vehiculo_causa_anio`. Esta columna la podrás calcular tras haber calculado las dos anteriores, sin necesidad de utilizar ninguna ventana.\n",
    "* Edad promedio de los accidentados *en ese mismo año con esa misma categoría de vehículo*, en una nueva columna `media_edad_vehiculo_anio`.\n",
    "* Edad promedio de los accidentados *en ese mismo año con esa misma categoría de vehículo y con esa misma situación (Junction_Location)*, en una nueva columna `media_edad_vehiculo_causa_anio`.\n",
    "* Desviación típica (función `F.stddev`) del dato anterior *en ese mismo año con ese mismo tipo de vehículo y con esa misma situación (Junction_Location)*, en una nuea columna `stddev_edad_vehiculo_causa_anio`.\n",
    "* Guardar el DF resultante en una nueva variable llamada `accidentes_info_agregadaDF`.\n",
    "\n",
    "PISTA: crear en las variables `ventana_vehiculo_anio` y `ventana_vehiculo_causa_anio` dos ventanas diferentes que definan los dos grupos distintos que intervienen en los cálculos anteriores (una de ellos con dos criterios y la otra con tres).\n",
    "\n",
    "**Revisa el diccionario de variables y su significado para saber qué columnas debes utilizar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3740ea97-2ce0-4309-9212-22ddfb94add5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b7e75daf483336d265c2afeeb3b343c",
     "grade": false,
     "grade_id": "windows",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Importo la clase Window de PySpark, que permite definir particiones (grupos)\n",
    "# sobre los cuales, aplicar funciones de ventana como count, avg o stddev sin agrupar el DataFrame.\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Creo dos ventanas, para aplicar agregaciones sin perder el detalle por fila:\n",
    "# 1. Agrupación por año y categoría del vehículo.\n",
    "ventana_vehiculo_anio = Window.partitionBy(\"Year\", \"Vehicle_Category\")\n",
    "\n",
    "# 2. Agrupación por año, categoría del vehículo y ubicación del accidente (Junction_Location).\n",
    "ventana_vehiculo_causa_anio = Window.partitionBy(\"Year\", \"Vehicle_Category\", \"Junction_Location\")\n",
    "\n",
    "# Enriquezco el DataFrame original, con estadísticas agregadas usando las ventanas definidas.\n",
    "accidentes_info_agregadaDF = (\n",
    "    accidentesDF\n",
    "    # Total de accidentes por año y categoría de vehículo\n",
    "    .withColumn(\"total_vehiculo_anio\", F.count(\"*\").over(ventana_vehiculo_anio))\n",
    "\n",
    "    # Total de accidentes por año, categoría de vehículo y ubicación del accidente.\n",
    "    .withColumn(\"total_vehiculo_causa_anio\", F.count(\"*\").over(ventana_vehiculo_causa_anio))\n",
    "\n",
    "    # Proporción (en tanto por uno) entre los accidentes con esa ubicación y el total del grupo.\n",
    "    .withColumn(\"porc_vehiculo_causa_anio\", F.col(\"total_vehiculo_causa_anio\") / F.col(\"total_vehiculo_anio\"))\n",
    "\n",
    "    # Edad promedio del conductor por año y tipo de vehículo.\n",
    "    .withColumn(\"media_edad_vehiculo_anio\", F.avg(\"Age_of_Driver\").over(ventana_vehiculo_anio))\n",
    "\n",
    "    # Edad promedio del conductor por año, tipo de vehículo y ubicación del accidente.\n",
    "    .withColumn(\"media_edad_vehiculo_causa_anio\", F.avg(\"Age_of_Driver\").over(ventana_vehiculo_causa_anio))\n",
    "\n",
    "    # Desviación estándar de la edad del conductor, en ese mismo grupo detallado.\n",
    "    .withColumn(\"stddev_edad_vehiculo_causa_anio\", F.stddev(\"Age_of_Driver\").over(ventana_vehiculo_causa_anio))\n",
    ")\n",
    "\n",
    "# COMENTARIO GENERAL DEL CÓDIGO:\n",
    "# Este bloque enriquece el DataFrame original ('accidentesDF') con nuevas columnas estadísticas,\n",
    "# calculadas mediante funciones de ventana. Estas funciones permiten obtener, para cada accidente:\n",
    "# - El número total de accidentes por año y tipo de vehículo,\n",
    "# - El número total de accidentes por año, tipo de vehículo y ubicación del accidente (Junction_Location),\n",
    "# - La proporción entre ambos (porcentaje en tanto por uno),\n",
    "# - La edad media y desviación estándar, del conductor dentro de esos grupos.\n",
    "# De este modo, se añade contexto agregado a cada fila, sin necesidad de agrupar ni perder detalle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cefa54a-dbda-4a81-ba45-7c5b2970a9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c774e9fb4432a2a46f881e26168e7381",
     "grade": true,
     "grade_id": "windows_tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "r = accidentes_info_agregadaDF.select(F.mean(\"total_vehiculo_anio\").alias(\"total_vehiculo_anio\"),\n",
    "                                  F.mean(\"total_vehiculo_causa_anio\").alias(\"total_vehiculo_causa_anio\"),\n",
    "                                  F.mean(\"porc_vehiculo_causa_anio\").alias(\"porc_vehiculo_causa_anio\"),\n",
    "                                  F.mean(\"media_edad_vehiculo_causa_anio\").alias(\"media_edad_vehiculo_causa_anio\"),\n",
    "                                 ).first()\n",
    "assert(round(r.total_vehiculo_anio, 2) == 33843.98)\n",
    "assert(round(r.total_vehiculo_causa_anio, 2) == 8185.52)\n",
    "assert(round(r.porc_vehiculo_causa_anio, 2) == 0.25)\n",
    "assert(round(r.media_edad_vehiculo_causa_anio, 2) == 3.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32813049-f39e-423e-8a80-9acc054a240a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e4076dfd2f060035bc93d59660edd10",
     "grade": false,
     "grade_id": "cell-fc88821f19453a51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 3 (1 punto)** Queremos saber si el tipo de vehículo está relacionado con la hora del día a la que se tienen más accidentes, y si es diferente entre cada tipo de vehículo. Para ello, partiendo de nuevo de `accidentesDF` \n",
    "* Crear un nuevo DF con tantas filas como horas del día existen, y tantas columnas como categorías de vehículo más una (que será justamente la hora). En cada casilla, debe contener el número de accidentes ocurridos a esa hora del día con ese tipo de vehículo.\n",
    "* Ordenar el DF en base a la hora de menor a mayor.\n",
    "* Almacenar el DF resultante en la variable `accidentes_hora_vehiculo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d465724-bafb-42a3-b35d-57dac5cd61b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe4b77eccb84199d2ee1209f64f95764",
     "grade": false,
     "grade_id": "accidentes_horas_edad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Construyo un nuevo DataFrame, que resume el número de accidentes por hora del día y tipo de vehículo.\n",
    "# La columna 'hora' actuará como índice, y cada tipo de vehículo será una columna separada.\n",
    "\n",
    "accidentes_hora_vehiculo = (\n",
    "    accidentesDF\n",
    "\n",
    "    # Agrupo los datos por la columna 'hora' (hora del día en que ocurrió el accidente).\n",
    "    .groupBy(\"hora\")\n",
    "\n",
    "    # Aplico un pivot sobre la columna 'Vehicle_Category' para que cada tipo de vehículo se convierta en una columna.\n",
    "    # Esto permite ver, en cada hora, cuántos accidentes hubo, de cada tipo de vehículo.\n",
    "    .pivot(\"Vehicle_Category\")\n",
    "    # Cuento cuántos accidentes hay en cada combinación, hora-tipo de vehículo.\n",
    "    .count()\n",
    "    # Ordeno las filas por la hora de forma ascendente (de 0 a 23).\n",
    "    .orderBy(\"hora\")\n",
    ")\n",
    "# COMENTARIO GENERAL DEL CÓDIGO:\n",
    "# Este bloque, analiza la distribución de accidentes según la hora del día y el tipo de vehículo.\n",
    "# Crea una tabla con una fila por hora (0 a 23) y una columna por categoría de vehículo,\n",
    "# donde cada celda contiene el número de accidentes ocurridos a esa hora, con ese tipo de vehículo.\n",
    "# El resultado permite comparar visualmente en qué franjas horarias cada tipo de vehículo sufre más accidentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7eead5-e0c6-4c0c-b475-bcd963b116fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74d20d67561b86e9d03d5d6b1463a153",
     "grade": true,
     "grade_id": "accidentes_horas_edad_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "acc = accidentes_hora_vehiculo.collect()\n",
    "assert(acc[0].hora == 0 and acc[0].Taxi == 324)\n",
    "assert(acc[10].hora == 10 and acc[10].Other == 41)\n",
    "assert(acc[15].hora == 15 and acc[15].Motorcycle == 1860)\n",
    "assert(acc[19].hora == 19 and acc[19].Car == 10886)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c0121d-46b0-4583-9fb9-84acd4a9d377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac7cddd3b4a27d28c49547bbf66191ea",
     "grade": false,
     "grade_id": "cell-b7957df90f6199a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 4 (1 punto)** Partiendo de la variable `accidentes_hora_vehiculo` creada en el ejercicio anterior, crear un nuevo DF de **una sola fila** y tantas columnas como categorías de vehículos (es decir, 6). Debe contener, para cada columna, una *pareja del número de accidentes máximo que ocurre a lo largo del día, y la hora a la que se produjeron*. Para ello, en lugar de ir aplicando la función `F.max` a cada columna del DF anterior (dentro de una llamada a `select`), aplícala en cada momento lo que devuelve la función `F.struct(nombreColumna, \"hora\"`), es decir, `F.max(F.struct(nombreColumna, \"hora\"))`. De esta forma, estarás creando (al vuelo) un objeto columna de parejas, cuyo primer elemento de cada pareja es el número total de accidentes indicado en esa columna, y cuyo segundo elemento es la hora del día a la que se ha producido. La función `F.max` aplicada a una columna de tipo parejas tendrá en cuenta, por defecto, solamente el primer elemento de cada pareja para ordenar, así que escogerá la pareja que tiene un mayor número de accidentes ya que ese valor es el primer elemento de cada pareja, pero lo mostrará como pareja, con lo que veremos la hora del día a la que va aparejado ese número de accidentes.\n",
    "\n",
    "Cada columna de pares mostrada por F.max debe renombrarse exactamente con el nombre de la categoría de vehículo a la que corresponde esa pareja. \n",
    "\n",
    "El DF resultante debe quedar guardado en la variable `hora_max_accidentes_vehiculo_df`\n",
    "\n",
    "PISTA: la solución es simplemente una operación `select` que incluye dentro la creación de 6 columnas al vuelo haciendo 6 llamadas a la función `F.max(F.struct(..., \"hora\"))`, y haciendo `alias` sobre el objeto columna devuelto por cada una de estas llamadas, para que cada una de esas 6 columnas creadas al vuelo se llame igual que su categoría de vehículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d1e94a-610c-4640-bb3b-815be7059e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1695635e6d3eaf0de79f854590ffe8f",
     "grade": false,
     "grade_id": "parejas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Creo un nuevo DataFrame, que tendrá una sola fila y 6 columnas (una por tipo de vehículo).\n",
    "# En cada columna, guardo una estructura (struct) con dos elementos:\n",
    "#  - El número máximo de accidentes registrados, para ese tipo de vehículo.\n",
    "#  - La hora del día (0-23), en que ocurrió ese máximo.\n",
    "\n",
    "hora_max_accidentes_vehiculo_df = accidentes_hora_vehiculo.select(\n",
    "\n",
    "    # Para \"Bus/minibus\", obtenego la fila con más accidentes y guardo el par (accidentes, hora).\n",
    "    F.max(F.struct(\"Bus/minibus\", \"hora\")).alias(\"Bus/minibus\"),\n",
    "\n",
    "     # Para \"Car\" hago lo mismo: elijo el struct, con más accidentes y la hora correspondiente.\n",
    "    F.max(F.struct(\"Car\", \"hora\")).alias(\"Car\"),\n",
    "\n",
    "     # Para \"Motorcycle\" hago lo mismo.\n",
    "    F.max(F.struct(\"Motorcycle\", \"hora\")).alias(\"Motorcycle\"),\n",
    "\n",
    "    # Para \"Other\" hago lo mismo.\n",
    "    F.max(F.struct(\"Other\", \"hora\")).alias(\"Other\"),\n",
    "\n",
    "    # Para \"Taxi\" hago lo mismo.\n",
    "    F.max(F.struct(\"Taxi\", \"hora\")).alias(\"Taxi\"),\n",
    "\n",
    "    # Para \"Van\" hago lo mismo.\n",
    "    F.max(F.struct(\"Van\", \"hora\")).alias(\"Van\")\n",
    ")\n",
    "# COMENTARIO GENERAL DEL CÓDIGO:\n",
    "# Este bloque calcula, para cada categoría de vehículo, la hora del día en la que se produce\n",
    "# el mayor número de accidentes. El resultado es un DataFrame de una sola fila, donde cada columna\n",
    "# contiene un par (número de accidentes, hora) correspondiente al pico máximo de siniestros diarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c808de47-eff3-41a2-8491-b66ebb702f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12eac985a78396cbb431c28fb982cb5f",
     "grade": true,
     "grade_id": "parejas_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(hora_max_accidentes_vehiculo_df.columns) == 6)\n",
    "assert(sum([1 for c in [\"Bus/minibus\", \"Car\", \"Motorcycle\", \"Other\", \"Taxi\", \"Van\"]\n",
    "          if c in hora_max_accidentes_vehiculo_df.columns]) == 6)\n",
    "r2 = hora_max_accidentes_vehiculo_df.first()\n",
    "assert(r2[\"Bus/minibus\"][0] == 56 and r2[\"Bus/minibus\"][1] == 15)\n",
    "assert(r2[\"Car\"][0] == 19961 and r2[\"Car\"][1] == 17)\n",
    "assert(r2[\"Motorcycle\"][0] == 2751 and r2[\"Motorcycle\"][1] == 17)\n",
    "assert(r2[\"Other\"][0] == 64 and r2[\"Other\"][1] == 13)\n",
    "assert(r2[\"Taxi\"][0] == 389 and r2[\"Taxi\"][1] == 16)\n",
    "assert(r2[\"Van\"][0] == 1233 and r2[\"Van\"][1] == 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3786b0f5-d94b-4a71-9df3-83952fc698ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fafa36482bcec7ffb2e3cdcab5432e1d",
     "grade": false,
     "grade_id": "cell-a71a6b17b1e0d613",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 5 (2 puntos)** Vamos a preprocesar algunas variables para prepararlas para un posible algoritmo predictivo. Partiendo de `accidentesDF` se pide:\n",
    "* Crear en la variable `journey_purpose_indexer` un StringIndexer para la variable \"Driver_Journey_Purpose\" y que cree una nueva columna de salida `purpose_indexed`. Debe ser capaz de lidiar con etiquetas nunca vistas a la hora de hacer la codificación de un nuevo dataset (que no se eliminen dichas filas ni tampoco salte un error).\n",
    "* Crear en la variable `cars_involved_binarizer` un Binarizer de la variable `Number_of_Vehicles` que tenga `threshold=2.5` puesto que en la mayoría de los accidentes están involucrados 1 o 2 coches. Queremos pasarla a una variable binaria donde el 0.0 represente justamente que ha habido 1 o 2 coches involucrados, y el 1.0 represente que ha habido 3 o más coches involucrados. El binarizador debe crear como salida una nueva columna llamada `number_vehicles_binarized`\n",
    "* Crear en la variable `vector_assembler` un VectorAssembler que colapse en una nueva columna de tipo vector las columnas `purpose_indexed`, `number_vehicles_binarized` y `Speed_limit`. La nueva columna debe llamarse `features`. \n",
    "* Crear en la variable `pipeline` un pipeline que contenga **exclusivamente** las tres etapas anteriores. **NO DEBE CONTENER NINGÚN ALGORITMO PREDICTIVO**.\n",
    "* \"Entrenar\" ese pipeline con el DF `accidentesDF` y guardar el resultado en la variable `pipeline_model`. **No debe hacerse ningún tipo de división de los datos en entrenamiento y test**. Aunque el método sea \"entrenar\", en realidad sólo estamos ajustando etapas de pre-procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c804688c-3b39-40c6-8efb-64bf5f93d6ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a1d5b178979e3f47a9983bc8fa1565",
     "grade": false,
     "grade_id": "numero_categorias",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Importo las clases específicas, de la librería pyspark.ml.feature que permiten transformar columnas,\n",
    "# en formatos que los modelos de Machine Learning pueden procesar.\n",
    "from pyspark.ml.feature import StringIndexer, Binarizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creo un StringIndexer, para transformar la columna \"Driver_Journey_Purpose\" (texto),\n",
    "# en una columna numérica llamada \"purpose_indexed\".\n",
    "# Uso handleInvalid=\"keep\" para evitar errores, si aparece una categoría no vista al entrenar.\n",
    "journey_purpose_indexer = StringIndexer(\n",
    "    inputCol=\"Driver_Journey_Purpose\", # Columna de entrada (string).\n",
    "    outputCol=\"purpose_indexed\",       # Columna de salida (númerica).\n",
    "    handleInvalid=\"keep\"               # Si hay valores nuevos en datos futuros, no da error.\n",
    ")\n",
    "\n",
    "# Creo un Binarizer, para convertir el número de vehículos en una variable binaria.\n",
    "# Si el número de vehículos es mayor a 2.5, se pone 1.0, si no, se pone 0.0.\n",
    "cars_involved_binarizer = Binarizer(\n",
    "    inputCol=\"Number_of_Vehicles\",           # Entrada: número de vehículos.\n",
    "    outputCol=\"number_vehicles_binarized\",   # Salida: 0.0 o 1.0.\n",
    "    threshold=2.5                            # Umbral: 3 o más vehículos → 1.0.\n",
    ")\n",
    "\n",
    "# Creo un VectorAssembler, el cual combina 3 columnas en una sola columna de vectores numéricos.\n",
    "# Esta columna se llama \"features\" y se usará para alimentar algoritmos de ML.\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=[\"purpose_indexed\", \"number_vehicles_binarized\", \"Speed_limit\"],  # Entradas.\n",
    "    outputCol=\"features\"                                                        # Salida.\n",
    ")\n",
    "\n",
    "# Hago la unión de los tres pasos anteriores, en un único pipeline.\n",
    "# El pipeline ejecutará esas transformaciones en secuencia.\n",
    "pipeline = Pipeline(stages=[\n",
    "    journey_purpose_indexer,     # 1. Converte texto a número.\n",
    "    cars_involved_binarizer,     # 2. Converte número de vehículos a binario.\n",
    "    vector_assembler             # 3. Junta las columnas en un solo vector.\n",
    "])\n",
    "\n",
    "# \"Entreno\" el pipeline con el DataFrame original.\n",
    "# No entrena un modelo, solo ajusta los transformadores, para que estén listos para aplicar.\n",
    "pipeline_model = pipeline.fit(accidentesDF)\n",
    "\n",
    "# COMENTARIO GENERAL DEL CÓDIGO:\n",
    "# Este bloque construye un pipeline de preprocesamiento sobre el DataFrame 'accidentesDF',\n",
    "# preparando las variables para su uso en modelos predictivos. Convierte una variable categórica\n",
    "# en índice numérico, transforma una variable continua en binaria, y combina todas en un vector\n",
    "# de entrada. Estas etapas se encapsulan en un Pipeline, que puede aplicarse fácilmente a nuevos datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50c2ebe-c7d6-4e66-a5f4-8793ed08179f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab721356e0aa5734558f0999cab364d",
     "grade": true,
     "grade_id": "numero_categorias_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, Binarizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "assert(isinstance(journey_purpose_indexer, StringIndexer))\n",
    "assert(journey_purpose_indexer.getInputCol() == \"Driver_Journey_Purpose\" and \n",
    "       journey_purpose_indexer.getOutputCol() == \"purpose_indexed\" and\n",
    "       journey_purpose_indexer.getHandleInvalid() == \"keep\")\n",
    "\n",
    "assert(isinstance(cars_involved_binarizer, Binarizer))\n",
    "assert(cars_involved_binarizer.getInputCol() == \"Number_of_Vehicles\" and \n",
    "       cars_involved_binarizer.getOutputCol() == \"number_vehicles_binarized\" and\n",
    "       cars_involved_binarizer.getThreshold() == 2.5)\n",
    "\n",
    "assert(isinstance(pipeline, Pipeline))\n",
    "assert(len(pipeline.getStages()) == 3)             # el pipeline debe tener solamente tres etapas\n",
    "assert(journey_purpose_indexer in pipeline.getStages() \n",
    "       and cars_involved_binarizer in pipeline.getStages() \n",
    "       and vector_assembler in pipeline.getStages())\n",
    "\n",
    "assert(isinstance(pipeline_model, PipelineModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208f1fcc-d2e4-4ccd-8f86-5c8a9b5a79ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c20156c0fe97d2976dfa3b10f3693be7",
     "grade": false,
     "grade_id": "cell-27e00b5fb188db3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ejercicio 6 (1.5 puntos)** Queremos ver cuál es la forma de transporte involucrada en más accidentes en cada región de Reino Unido. Para ello, partiendo de `accidentesDF` se pide:\n",
    "\n",
    "* Crear un DF con tantas filas como Regiones distintas existen y tantas columnas como categorías de vehículo más una (la de la región, que estará a la izquierda). En cada casilla debe calcularse una **tripleta** (columna de tipo estructura, que se crea con `F.struct(col1, col2, col3)`) formada por:\n",
    "  * Número de accidentes en esa región y tipo vehículo,\n",
    "  * Edad media del conductor redondeada a 2 cifras decimales, y\n",
    "  * Número medio de coches involucrados redondeado a 2 cifras decimales). \n",
    "* Ordenar el DF alfabéticamente de menor a mayor en base a la columna `\"Region\"`\n",
    "* Guardar el DF resultante en la variable `numero_edad_coches_df`.\n",
    "* Para visualizarlo mejor, y puesto que el tamaño del DF que hemos obtenido como resultado está acotado por el número de regiones distintas existentes y por el número de categorías de vehículos existentes, pasar dicho DF a un dataframe de Pandas en la variable `numero_edad_coches_pd` y mostrarlo por pantalla.\n",
    "\n",
    "PISTA: para construir la columna de tipo estructura dentro de la función `agg(...)` se puede utilizar `F.struct(F.funcionagregacion(...), F.funcionagregacion(...), F.funcionagregacion(...))`. \n",
    "\n",
    "PISTA: en vez de pasarle a la función `F.struct` directamente la columna resultante de la agregación, pásale en caso necesario `F.round(F.nombrefuncion(...), 2)` para que ya esté redondeada.\n",
    "\n",
    "En el resultado puedes observar fenómenos como por ejemplo: \n",
    "* Los conductores de autobús son los que en promedio tienen siempre más edad, mientras que los de moto son los más jóvenes, como era de esperar. \n",
    "* Los accidentes de moto son los que menos coches involucran en promedio, en torno a 1.80, lo que quiere decir que hay muchos accidentes que los tiene el propio conductor sin que intervenga otro vehículo (condiciones atmosféricas, etc). Los de Taxi parecen estar bastante por debajo que los accidentes de coche, lo que indica que, mientras que un accidente de coche con frecuencia implica la interacción con otro vehículo, en los taxis hay aún bastantes accidentes donde no necesariamente hay otro vehículo implicado y por eso el promedio todavía no se acerca tanto a 2.\n",
    "* Es llamativo que en Gales haya unos promedios tan bajos en el número de vehículos involucrados en todas las categorías, en especial en los accidentes de coche y moto, lo que indica que muchos accidentes se producen sin causa de otro vehículo. Puede tener relación directa con que la edad media de los conductores es bastante superior a la de otros medios, y por eso son propensos a tener un accidente por mala conducción o relacionado con las facultades físicas o cognitivas del conductor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3947ffa-1ab3-4e3c-a6a8-f3b554a6d346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "390b166c02a0c1d81753a9ac3a404fc8",
     "grade": false,
     "grade_id": "tripletas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Region                               Bus/minibus  \\\n",
      "0               East England  {'col1': 44, 'col2': 4.34, 'col3': 1.77}   \n",
      "1              East Midlands  {'col1': 39, 'col2': 4.95, 'col3': 1.85}   \n",
      "2                     London  {'col1': 28, 'col2': 4.71, 'col3': 1.75}   \n",
      "3         North East England  {'col1': 52, 'col2': 4.46, 'col3': 1.85}   \n",
      "4         North West England   {'col1': 66, 'col2': 4.7, 'col3': 1.88}   \n",
      "5                   Scotland     {'col1': 2, 'col2': 4.0, 'col3': 1.5}   \n",
      "6         South East England  {'col1': 71, 'col2': 4.96, 'col3': 1.87}   \n",
      "7         South West England  {'col1': 37, 'col2': 4.84, 'col3': 1.81}   \n",
      "8                      Wales     {'col1': 1, 'col2': 5.0, 'col3': 2.0}   \n",
      "9              Wast Midlands   {'col1': 56, 'col2': 4.7, 'col3': 1.91}   \n",
      "10  Yorkshire and the Humber  {'col1': 75, 'col2': 4.67, 'col3': 1.69}   \n",
      "\n",
      "                                            Car  \\\n",
      "0    {'col1': 22813, 'col2': 3.95, 'col3': 2.0}   \n",
      "1   {'col1': 17191, 'col2': 3.91, 'col3': 1.96}   \n",
      "2    {'col1': 24015, 'col2': 3.9, 'col3': 1.92}   \n",
      "3    {'col1': 9272, 'col2': 3.96, 'col3': 1.96}   \n",
      "4   {'col1': 25783, 'col2': 4.01, 'col3': 1.97}   \n",
      "5     {'col1': 198, 'col2': 4.24, 'col3': 1.45}   \n",
      "6   {'col1': 39410, 'col2': 4.03, 'col3': 2.02}   \n",
      "7   {'col1': 21144, 'col2': 4.11, 'col3': 1.99}   \n",
      "8     {'col1': 282, 'col2': 4.23, 'col3': 1.73}   \n",
      "9   {'col1': 20933, 'col2': 3.85, 'col3': 1.97}   \n",
      "10  {'col1': 23049, 'col2': 3.93, 'col3': 1.96}   \n",
      "\n",
      "                                    Motorcycle  \\\n",
      "0    {'col1': 2753, 'col2': 3.17, 'col3': 1.8}   \n",
      "1   {'col1': 2084, 'col2': 3.29, 'col3': 1.79}   \n",
      "2   {'col1': 5658, 'col2': 3.22, 'col3': 1.82}   \n",
      "3    {'col1': 742, 'col2': 3.46, 'col3': 1.79}   \n",
      "4   {'col1': 2724, 'col2': 3.29, 'col3': 1.82}   \n",
      "5     {'col1': 65, 'col2': 4.77, 'col3': 1.25}   \n",
      "6    {'col1': 5281, 'col2': 3.28, 'col3': 1.8}   \n",
      "7   {'col1': 2802, 'col2': 3.17, 'col3': 1.88}   \n",
      "8     {'col1': 79, 'col2': 4.35, 'col3': 1.51}   \n",
      "9   {'col1': 2168, 'col2': 3.16, 'col3': 1.88}   \n",
      "10  {'col1': 2537, 'col2': 3.27, 'col3': 1.82}   \n",
      "\n",
      "                                       Other  \\\n",
      "0   {'col1': 61, 'col2': 4.02, 'col3': 1.87}   \n",
      "1   {'col1': 48, 'col2': 4.44, 'col3': 1.92}   \n",
      "2   {'col1': 56, 'col2': 3.89, 'col3': 1.84}   \n",
      "3   {'col1': 32, 'col2': 3.81, 'col3': 2.06}   \n",
      "4   {'col1': 66, 'col2': 4.47, 'col3': 1.98}   \n",
      "5      {'col1': 2, 'col2': 7.0, 'col3': 1.5}   \n",
      "6   {'col1': 145, 'col2': 3.81, 'col3': 2.0}   \n",
      "7    {'col1': 67, 'col2': 4.49, 'col3': 2.0}   \n",
      "8      {'col1': 2, 'col2': 2.5, 'col3': 1.5}   \n",
      "9   {'col1': 52, 'col2': 3.96, 'col3': 1.92}   \n",
      "10  {'col1': 75, 'col2': 3.92, 'col3': 1.99}   \n",
      "\n",
      "                                            Taxi  \\\n",
      "0    {'col1': 424.0, 'col2': 4.43, 'col3': 1.89}   \n",
      "1    {'col1': 362.0, 'col2': 4.36, 'col3': 1.83}   \n",
      "2   {'col1': 1671.0, 'col2': 4.63, 'col3': 1.79}   \n",
      "3    {'col1': 279.0, 'col2': 4.12, 'col3': 1.67}   \n",
      "4   {'col1': 1070.0, 'col2': 4.33, 'col3': 1.75}   \n",
      "5        {'col1': 1.0, 'col2': 5.0, 'col3': 1.0}   \n",
      "6    {'col1': 787.0, 'col2': 4.42, 'col3': 1.78}   \n",
      "7    {'col1': 317.0, 'col2': 4.58, 'col3': 1.74}   \n",
      "8                                           None   \n",
      "9    {'col1': 607.0, 'col2': 4.14, 'col3': 1.79}   \n",
      "10   {'col1': 742.0, 'col2': 4.15, 'col3': 1.75}   \n",
      "\n",
      "                                           Van  \n",
      "0   {'col1': 1470, 'col2': 3.91, 'col3': 2.12}  \n",
      "1   {'col1': 1117, 'col2': 3.98, 'col3': 2.06}  \n",
      "2   {'col1': 2212, 'col2': 3.88, 'col3': 1.91}  \n",
      "3     {'col1': 639, 'col2': 3.95, 'col3': 2.0}  \n",
      "4   {'col1': 1387, 'col2': 3.91, 'col3': 2.01}  \n",
      "5      {'col1': 26, 'col2': 4.0, 'col3': 1.46}  \n",
      "6    {'col1': 2548, 'col2': 3.92, 'col3': 2.1}  \n",
      "7   {'col1': 1236, 'col2': 3.97, 'col3': 2.03}  \n",
      "8     {'col1': 29, 'col2': 3.52, 'col3': 1.83}  \n",
      "9   {'col1': 1436, 'col2': 3.91, 'col3': 2.03}  \n",
      "10  {'col1': 1412, 'col2': 3.92, 'col3': 2.02}  \n"
     ]
    }
   ],
   "source": [
    "# Agrupo el DataFrame original, por Región y Tipo de Vehículo\n",
    "# y calculo tres métricas para cada combinación:\n",
    "# 1. Número total de accidentes.\n",
    "# 2. Edad promedio del conductor (redondeada a 2 decimales).\n",
    "# 3. Promedio de vehículos implicados (redondeado a 2 decimales).\n",
    "# Luego empaqueto estas 3 métricas, en una sola estructura (struct).\n",
    "grupo = accidentesDF.groupBy(\"Region\", \"Vehicle_Category\").agg(\n",
    "    F.struct(\n",
    "        F.count(\"*\"),                               # Total de accidentes.\n",
    "        F.round(F.avg(\"Age_of_Driver\"), 2),         # Edad media del conductor.\n",
    "        F.round(F.avg(\"Number_of_Vehicles\"), 2)     # Promedio de vehículos implicados.\n",
    "    ).alias(\"resumen\")                              # Le doy el nombre \"resumen\" a esta estructura.\n",
    ")\n",
    "\n",
    "# A partir del resultado anterior, hago la agrupación solo por Región\n",
    "# y aplico pivot, para convertir los tipos de vehículos en columnas individuales.\n",
    "# Para cada combinación, selecciono el primer valor del resumen (ya que solo hay uno por grupo).\n",
    "numero_edad_coches_df = (\n",
    "    grupo\n",
    "    .groupBy(\"Region\")           # Agrupo solo por región\n",
    "    .pivot(\"Vehicle_Category\")   # Cada tipo de vehículo, será una columna.\n",
    "    .agg(F.first(\"resumen\"))     # Tomo la estructura calculada, para cada tipo\n",
    "    .orderBy(\"Region\")           # Ordeno alfabéticamente, por Región.\n",
    ")\n",
    "\n",
    "# Convierto el DataFrame de Spark, a un DataFrame de Pandas\n",
    "# para poder visualizarlo más fácilmente, como tabla o exportarlo si fuera necesario.\n",
    "numero_edad_coches_pd = numero_edad_coches_df.toPandas()\n",
    "\n",
    "# Mostro por pantalla el resultado final en formato Pandas.\n",
    "# Cada celda contiene una tupla: (nº de accidentes, edad media, nº vehículos)\n",
    "print(numero_edad_coches_pd)\n",
    "\n",
    "# COMENTARIO GENERAL DEL CÓDIGO:\n",
    "# Este bloque analiza los accidentes por región y tipo de vehículo.\n",
    "# Para cada combinación se calcula el número de accidentes, la edad media del conductor\n",
    "# y el promedio de vehículos implicados. Estos datos se empaquetan en estructuras (structs)\n",
    "# y se pivotan para formar un resumen por región, con columnas separadas por tipo de vehículo.\n",
    "# Finalmente, se convierte a Pandas para facilitar su visualización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e335db-3092-4a46-913e-ac9f0446512c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a55def89481fb11a028a5f098d25f383",
     "grade": true,
     "grade_id": "tripletas_test",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(numero_edad_coches_df.columns) == 7)\n",
    "assert(sum([1 for c in [\"Region\", \"Bus/minibus\", \"Car\", \"Motorcycle\", \"Other\", \"Taxi\", \"Van\"]\n",
    "          if c in numero_edad_coches_df.columns]) == 7)\n",
    "r = numero_edad_coches_df.collect()\n",
    "assert(r[0].Region == \"East England\" and r[0].Other == (61, 4.02, 1.87))\n",
    "assert(len(numero_edad_coches_df.columns) == 7 and len(r) == 11)\n",
    "assert(r[0].Car == (22813, 3.95, 2.0))\n",
    "assert(r[7].Motorcycle == (2802, 3.17, 1.88))\n",
    "assert(r[10].Van == (1412, 3.92, 2.02))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6584647907221975,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_m6",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
