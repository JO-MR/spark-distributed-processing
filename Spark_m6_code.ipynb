{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "08db3b6a-9c7f-4b17-810e-e8494956f20f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3d01e43162b64c90ce0048e8a23f3b1b",
          "grade": false,
          "grade_id": "cell-f8987996be9f1238",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5jvEcUPgGHJz"
      },
      "source": [
        "# Accidentes de tráfico en Reino Unido entre 2010 y 2014\n",
        "\n",
        "### Disponible en Kaggle en:\n",
        "https://www.kaggle.com/stefanoleone992/adm-project-road-accidents-in-uk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2fbc52c4-73a9-49b7-8473-131f5409d5ed",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9a6b4dc108ddf890c659e33701965428",
          "grade": false,
          "grade_id": "cell-f74d7bfd01811789",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "kKMa0IMWGHJ0"
      },
      "source": [
        "### Variables y significado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3992d775-4517-40ab-b5c5-f41eaf395374",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4a5a5882319ae0a14393c8d534816a56",
          "grade": false,
          "grade_id": "cell-9cfb34982bd4eb04",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "wjwp2_cRGHJ1"
      },
      "source": [
        "* Accident_Index: Accident index\n",
        "* Latitude: Accident latitude\n",
        "* Longitude: Accident longitude\n",
        "* Region: Accident region\n",
        "* Urban_or_Rural_Area: Accident area (rural or urban)\n",
        "* X1st_Road_Class: Accident road class\n",
        "* Driver_IMD_Decile: Road IMD Decile\n",
        "* Speed_limit: Road speed limit\n",
        "* Road_Type: Road type\n",
        "* Road_Surface_Conditions: Road surface condition\n",
        "* Weather: Weather\n",
        "* High_Wind: High wind\n",
        "* Lights: Road lights\n",
        "* Datetime: Accident datetime\n",
        "* Year: Accident year\n",
        "* Season: Accident season\n",
        "* Month_of_Year: Accident month\n",
        "* Day_of_Month: Accident day of month\n",
        "* Day_of_Week: Accident day of week\n",
        "* Hour_of_Day: Accident hour of day\n",
        "* Number_of_Vehicles: Accident number of vehicles\n",
        "* Age_of_Driver: Driver age\n",
        "* Age_of_Vehicle: Vehicle age\n",
        "* Junction_Detail: Accident junction detail\n",
        "* Junction_Location: Accident junction location\n",
        "* X1st_Point_of_Impact: Vehicle first point of impact\n",
        "* Driver_Journey_Purpose: Driver journey purpose\n",
        "* Engine_CC: Vehicle engine power (in CC)\n",
        "* Propulsion_Code: Vehicle propulsion code\n",
        "* Vehicle_Make: Vehicle brand\n",
        "* Vehicle_Category: Vehicle brand category\n",
        "* Vehicle_Manoeuvre: Vehicle manoeuvre when accident happened\n",
        "* Accident_Severity: Accident severity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4bc88526-bb53-43c5-bb05-0c0423e78b7f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "OoInS4ckI_b_"
      },
      "outputs": [],
      "source": [
        "# Importo el módulo de funciones de PySpark SQL, que contiene transformaciones como when, col, hour, count, etc.\n",
        "# Uso el alias 'F' para referirme a esas funciones de forma abreviada (por ejemplo: F.col(\"columna\")).\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Configuración para acceder a Azure Data Lake Storage (ADLS Gen2)\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.masterjmr.dfs.core.windows.net\",\n",
        "    \"<REDACTED_ACCESS_KEY>\"\n",
        ")\n",
        "\n",
        "# Ruta del archivo CSV en ADLS\n",
        "ruta_csv = \"abfss://datos@masterjmr.dfs.core.windows.net/accidents_uk.csv\"\n",
        "\n",
        "# Lectura del CSV desde ADLS, con inferencia de tipos\n",
        "accidentesDF = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(ruta_csv)\n",
        "\n",
        "    # Creo una nueva columna llamada 'Age_Category' clasificando la edad del conductor en 4 grupos.\n",
        "    .withColumn(\n",
        "        \"Age_Category\",\n",
        "        F.when(F.col(\"Age_of_Driver\").isin(1, 2), \"Adolescente\")\n",
        "         .when(F.col(\"Age_of_Driver\").isin(3, 4), \"Joven\")\n",
        "         .when(F.col(\"Age_of_Driver\").isin(5, 6), \"Adulto\")\n",
        "         .when(F.col(\"Age_of_Driver\").isin(7, 8), \"Anciano\")\n",
        "    )\n",
        "\n",
        "    # Extraigo la hora del accidente, desde la columna 'Datetime'.\n",
        "    .withColumn(\"hora\", F.hour(\"Datetime\"))\n",
        "\n",
        "    # Cacheo el DataFrame, dado que se reutilizará en ejercicios posteriores.\n",
        "    .cache()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "abb9cdef-024a-487c-be17-264a0cc4ac19",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "214402d3a6c78ca57ae876cb84f7e722",
          "grade": true,
          "grade_id": "read_csv_test",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "9njiFGwwI_cC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "assert(accidentesDF.schema[1].dataType == DoubleType())\n",
        "assert(accidentesDF.count() == 251832)\n",
        "\n",
        "assert(dict(accidentesDF.dtypes)[\"Age_Category\"] == \"string\")\n",
        "collectedDF = accidentesDF.groupBy(\"Age_Category\").count().orderBy(\"count\").collect()\n",
        "assert((collectedDF[0][\"count\"] == 22533) & (collectedDF[0][\"Age_Category\"] == \"Anciano\"))\n",
        "assert((collectedDF[1][\"count\"] == 57174) & (collectedDF[1][\"Age_Category\"] == \"Adolescente\"))\n",
        "assert((collectedDF[2][\"count\"] == 67138) & (collectedDF[2][\"Age_Category\"] == \"Adulto\"))\n",
        "assert((collectedDF[3][\"count\"] == 104987) & (collectedDF[3][\"Age_Category\"] == \"Joven\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3740ea97-2ce0-4309-9212-22ddfb94add5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2b7e75daf483336d265c2afeeb3b343c",
          "grade": false,
          "grade_id": "windows",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "yylF-b0aI_cC"
      },
      "outputs": [],
      "source": [
        "# Importo la clase Window de PySpark, que permite definir particiones (grupos)\n",
        "# sobre los cuales, aplicar funciones de ventana como count, avg o stddev sin agrupar el DataFrame.\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Creo dos ventanas, para aplicar agregaciones sin perder el detalle por fila:\n",
        "# 1. Agrupación por año y categoría del vehículo.\n",
        "ventana_vehiculo_anio = Window.partitionBy(\"Year\", \"Vehicle_Category\")\n",
        "\n",
        "# 2. Agrupación por año, categoría del vehículo y ubicación del accidente (Junction_Location).\n",
        "ventana_vehiculo_causa_anio = Window.partitionBy(\"Year\", \"Vehicle_Category\", \"Junction_Location\")\n",
        "\n",
        "# Enriquezco el DataFrame original, con estadísticas agregadas usando las ventanas definidas.\n",
        "accidentes_info_agregadaDF = (\n",
        "    accidentesDF\n",
        "    # Total de accidentes por año y categoría de vehículo\n",
        "    .withColumn(\"total_vehiculo_anio\", F.count(\"*\").over(ventana_vehiculo_anio))\n",
        "\n",
        "    # Total de accidentes por año, categoría de vehículo y ubicación del accidente.\n",
        "    .withColumn(\"total_vehiculo_causa_anio\", F.count(\"*\").over(ventana_vehiculo_causa_anio))\n",
        "\n",
        "    # Proporción (en tanto por uno) entre los accidentes con esa ubicación y el total del grupo.\n",
        "    .withColumn(\"porc_vehiculo_causa_anio\", F.col(\"total_vehiculo_causa_anio\") / F.col(\"total_vehiculo_anio\"))\n",
        "\n",
        "    # Edad promedio del conductor por año y tipo de vehículo.\n",
        "    .withColumn(\"media_edad_vehiculo_anio\", F.avg(\"Age_of_Driver\").over(ventana_vehiculo_anio))\n",
        "\n",
        "    # Edad promedio del conductor por año, tipo de vehículo y ubicación del accidente.\n",
        "    .withColumn(\"media_edad_vehiculo_causa_anio\", F.avg(\"Age_of_Driver\").over(ventana_vehiculo_causa_anio))\n",
        "\n",
        "    # Desviación estándar de la edad del conductor, en ese mismo grupo detallado.\n",
        "    .withColumn(\"stddev_edad_vehiculo_causa_anio\", F.stddev(\"Age_of_Driver\").over(ventana_vehiculo_causa_anio))\n",
        ")\n",
        "\n",
        "# COMENTARIO GENERAL DEL CÓDIGO:\n",
        "# Este bloque enriquece el DataFrame original ('accidentesDF') con nuevas columnas estadísticas,\n",
        "# calculadas mediante funciones de ventana. Estas funciones permiten obtener, para cada accidente:\n",
        "# - El número total de accidentes por año y tipo de vehículo,\n",
        "# - El número total de accidentes por año, tipo de vehículo y ubicación del accidente (Junction_Location),\n",
        "# - La proporción entre ambos (porcentaje en tanto por uno),\n",
        "# - La edad media y desviación estándar, del conductor dentro de esos grupos.\n",
        "# De este modo, se añade contexto agregado a cada fila, sin necesidad de agrupar ni perder detalle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1cefa54a-dbda-4a81-ba45-7c5b2970a9ae",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c774e9fb4432a2a46f881e26168e7381",
          "grade": true,
          "grade_id": "windows_tests",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "98bXGEVRI_cD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "r = accidentes_info_agregadaDF.select(F.mean(\"total_vehiculo_anio\").alias(\"total_vehiculo_anio\"),\n",
        "                                  F.mean(\"total_vehiculo_causa_anio\").alias(\"total_vehiculo_causa_anio\"),\n",
        "                                  F.mean(\"porc_vehiculo_causa_anio\").alias(\"porc_vehiculo_causa_anio\"),\n",
        "                                  F.mean(\"media_edad_vehiculo_causa_anio\").alias(\"media_edad_vehiculo_causa_anio\"),\n",
        "                                 ).first()\n",
        "assert(round(r.total_vehiculo_anio, 2) == 33843.98)\n",
        "assert(round(r.total_vehiculo_causa_anio, 2) == 8185.52)\n",
        "assert(round(r.porc_vehiculo_causa_anio, 2) == 0.25)\n",
        "assert(round(r.media_edad_vehiculo_causa_anio, 2) == 3.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1d465724-bafb-42a3-b35d-57dac5cd61b5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe4b77eccb84199d2ee1209f64f95764",
          "grade": false,
          "grade_id": "accidentes_horas_edad",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "zbXEyEGuI_cD"
      },
      "outputs": [],
      "source": [
        "# Construyo un nuevo DataFrame, que resume el número de accidentes por hora del día y tipo de vehículo.\n",
        "# La columna 'hora' actuará como índice, y cada tipo de vehículo será una columna separada.\n",
        "\n",
        "accidentes_hora_vehiculo = (\n",
        "    accidentesDF\n",
        "\n",
        "    # Agrupo los datos por la columna 'hora' (hora del día en que ocurrió el accidente).\n",
        "    .groupBy(\"hora\")\n",
        "\n",
        "    # Aplico un pivot sobre la columna 'Vehicle_Category' para que cada tipo de vehículo se convierta en una columna.\n",
        "    # Esto permite ver, en cada hora, cuántos accidentes hubo, de cada tipo de vehículo.\n",
        "    .pivot(\"Vehicle_Category\")\n",
        "    # Cuento cuántos accidentes hay en cada combinación, hora-tipo de vehículo.\n",
        "    .count()\n",
        "    # Ordeno las filas por la hora de forma ascendente (de 0 a 23).\n",
        "    .orderBy(\"hora\")\n",
        ")\n",
        "# COMENTARIO GENERAL DEL CÓDIGO:\n",
        "# Este bloque, analiza la distribución de accidentes según la hora del día y el tipo de vehículo.\n",
        "# Crea una tabla con una fila por hora (0 a 23) y una columna por categoría de vehículo,\n",
        "# donde cada celda contiene el número de accidentes ocurridos a esa hora, con ese tipo de vehículo.\n",
        "# El resultado permite comparar visualmente en qué franjas horarias cada tipo de vehículo sufre más accidentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2e7eead5-e0c6-4c0c-b475-bcd963b116fa",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "74d20d67561b86e9d03d5d6b1463a153",
          "grade": true,
          "grade_id": "accidentes_horas_edad_test",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Pku9pUAUI_cD"
      },
      "outputs": [],
      "source": [
        "acc = accidentes_hora_vehiculo.collect()\n",
        "assert(acc[0].hora == 0 and acc[0].Taxi == 324)\n",
        "assert(acc[10].hora == 10 and acc[10].Other == 41)\n",
        "assert(acc[15].hora == 15 and acc[15].Motorcycle == 1860)\n",
        "assert(acc[19].hora == 19 and acc[19].Car == 10886)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "98d1e94a-610c-4640-bb3b-815be7059e49",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a1695635e6d3eaf0de79f854590ffe8f",
          "grade": false,
          "grade_id": "parejas",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "eKJpizb5I_cE"
      },
      "outputs": [],
      "source": [
        "# Creo un nuevo DataFrame, que tendrá una sola fila y 6 columnas (una por tipo de vehículo).\n",
        "# En cada columna, guardo una estructura (struct) con dos elementos:\n",
        "#  - El número máximo de accidentes registrados, para ese tipo de vehículo.\n",
        "#  - La hora del día (0-23), en que ocurrió ese máximo.\n",
        "\n",
        "hora_max_accidentes_vehiculo_df = accidentes_hora_vehiculo.select(\n",
        "\n",
        "    # Para \"Bus/minibus\", obtenego la fila con más accidentes y guardo el par (accidentes, hora).\n",
        "    F.max(F.struct(\"Bus/minibus\", \"hora\")).alias(\"Bus/minibus\"),\n",
        "\n",
        "     # Para \"Car\" hago lo mismo: elijo el struct, con más accidentes y la hora correspondiente.\n",
        "    F.max(F.struct(\"Car\", \"hora\")).alias(\"Car\"),\n",
        "\n",
        "     # Para \"Motorcycle\" hago lo mismo.\n",
        "    F.max(F.struct(\"Motorcycle\", \"hora\")).alias(\"Motorcycle\"),\n",
        "\n",
        "    # Para \"Other\" hago lo mismo.\n",
        "    F.max(F.struct(\"Other\", \"hora\")).alias(\"Other\"),\n",
        "\n",
        "    # Para \"Taxi\" hago lo mismo.\n",
        "    F.max(F.struct(\"Taxi\", \"hora\")).alias(\"Taxi\"),\n",
        "\n",
        "    # Para \"Van\" hago lo mismo.\n",
        "    F.max(F.struct(\"Van\", \"hora\")).alias(\"Van\")\n",
        ")\n",
        "# COMENTARIO GENERAL DEL CÓDIGO:\n",
        "# Este bloque calcula, para cada categoría de vehículo, la hora del día en la que se produce\n",
        "# el mayor número de accidentes. El resultado es un DataFrame de una sola fila, donde cada columna\n",
        "# contiene un par (número de accidentes, hora) correspondiente al pico máximo de siniestros diarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c808de47-eff3-41a2-8491-b66ebb702f95",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12eac985a78396cbb431c28fb982cb5f",
          "grade": true,
          "grade_id": "parejas_test",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7okHO174I_cE"
      },
      "outputs": [],
      "source": [
        "assert(len(hora_max_accidentes_vehiculo_df.columns) == 6)\n",
        "assert(sum([1 for c in [\"Bus/minibus\", \"Car\", \"Motorcycle\", \"Other\", \"Taxi\", \"Van\"]\n",
        "          if c in hora_max_accidentes_vehiculo_df.columns]) == 6)\n",
        "r2 = hora_max_accidentes_vehiculo_df.first()\n",
        "assert(r2[\"Bus/minibus\"][0] == 56 and r2[\"Bus/minibus\"][1] == 15)\n",
        "assert(r2[\"Car\"][0] == 19961 and r2[\"Car\"][1] == 17)\n",
        "assert(r2[\"Motorcycle\"][0] == 2751 and r2[\"Motorcycle\"][1] == 17)\n",
        "assert(r2[\"Other\"][0] == 64 and r2[\"Other\"][1] == 13)\n",
        "assert(r2[\"Taxi\"][0] == 389 and r2[\"Taxi\"][1] == 16)\n",
        "assert(r2[\"Van\"][0] == 1233 and r2[\"Van\"][1] == 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c804688c-3b39-40c6-8efb-64bf5f93d6ea",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a3a1d5b178979e3f47a9983bc8fa1565",
          "grade": false,
          "grade_id": "numero_categorias",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "znK-gGhtI_cE"
      },
      "outputs": [],
      "source": [
        "# Importo las clases específicas, de la librería pyspark.ml.feature que permiten transformar columnas,\n",
        "# en formatos que los modelos de Machine Learning pueden procesar.\n",
        "from pyspark.ml.feature import StringIndexer, Binarizer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Creo un StringIndexer, para transformar la columna \"Driver_Journey_Purpose\" (texto),\n",
        "# en una columna numérica llamada \"purpose_indexed\".\n",
        "# Uso handleInvalid=\"keep\" para evitar errores, si aparece una categoría no vista al entrenar.\n",
        "journey_purpose_indexer = StringIndexer(\n",
        "    inputCol=\"Driver_Journey_Purpose\", # Columna de entrada (string).\n",
        "    outputCol=\"purpose_indexed\",       # Columna de salida (númerica).\n",
        "    handleInvalid=\"keep\"               # Si hay valores nuevos en datos futuros, no da error.\n",
        ")\n",
        "\n",
        "# Creo un Binarizer, para convertir el número de vehículos en una variable binaria.\n",
        "# Si el número de vehículos es mayor a 2.5, se pone 1.0, si no, se pone 0.0.\n",
        "cars_involved_binarizer = Binarizer(\n",
        "    inputCol=\"Number_of_Vehicles\",           # Entrada: número de vehículos.\n",
        "    outputCol=\"number_vehicles_binarized\",   # Salida: 0.0 o 1.0.\n",
        "    threshold=2.5                            # Umbral: 3 o más vehículos → 1.0.\n",
        ")\n",
        "\n",
        "# Creo un VectorAssembler, el cual combina 3 columnas en una sola columna de vectores numéricos.\n",
        "# Esta columna se llama \"features\" y se usará para alimentar algoritmos de ML.\n",
        "vector_assembler = VectorAssembler(\n",
        "    inputCols=[\"purpose_indexed\", \"number_vehicles_binarized\", \"Speed_limit\"],  # Entradas.\n",
        "    outputCol=\"features\"                                                        # Salida.\n",
        ")\n",
        "\n",
        "# Hago la unión de los tres pasos anteriores, en un único pipeline.\n",
        "# El pipeline ejecutará esas transformaciones en secuencia.\n",
        "pipeline = Pipeline(stages=[\n",
        "    journey_purpose_indexer,     # 1. Converte texto a número.\n",
        "    cars_involved_binarizer,     # 2. Converte número de vehículos a binario.\n",
        "    vector_assembler             # 3. Junta las columnas en un solo vector.\n",
        "])\n",
        "\n",
        "# \"Entreno\" el pipeline con el DataFrame original.\n",
        "# No entrena un modelo, solo ajusta los transformadores, para que estén listos para aplicar.\n",
        "pipeline_model = pipeline.fit(accidentesDF)\n",
        "\n",
        "# COMENTARIO GENERAL DEL CÓDIGO:\n",
        "# Este bloque construye un pipeline de preprocesamiento sobre el DataFrame 'accidentesDF',\n",
        "# preparando las variables para su uso en modelos predictivos. Convierte una variable categórica\n",
        "# en índice numérico, transforma una variable continua en binaria, y combina todas en un vector\n",
        "# de entrada. Estas etapas se encapsulan en un Pipeline, que puede aplicarse fácilmente a nuevos datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d50c2ebe-c7d6-4e66-a5f4-8793ed08179f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4ab721356e0aa5734558f0999cab364d",
          "grade": true,
          "grade_id": "numero_categorias_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f-0jlZYVI_cF"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, Binarizer, VectorAssembler\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "assert(isinstance(journey_purpose_indexer, StringIndexer))\n",
        "assert(journey_purpose_indexer.getInputCol() == \"Driver_Journey_Purpose\" and\n",
        "       journey_purpose_indexer.getOutputCol() == \"purpose_indexed\" and\n",
        "       journey_purpose_indexer.getHandleInvalid() == \"keep\")\n",
        "\n",
        "assert(isinstance(cars_involved_binarizer, Binarizer))\n",
        "assert(cars_involved_binarizer.getInputCol() == \"Number_of_Vehicles\" and\n",
        "       cars_involved_binarizer.getOutputCol() == \"number_vehicles_binarized\" and\n",
        "       cars_involved_binarizer.getThreshold() == 2.5)\n",
        "\n",
        "assert(isinstance(pipeline, Pipeline))\n",
        "assert(len(pipeline.getStages()) == 3)             # el pipeline debe tener solamente tres etapas\n",
        "assert(journey_purpose_indexer in pipeline.getStages()\n",
        "       and cars_involved_binarizer in pipeline.getStages()\n",
        "       and vector_assembler in pipeline.getStages())\n",
        "\n",
        "assert(isinstance(pipeline_model, PipelineModel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a3947ffa-1ab3-4e3c-a6a8-f3b554a6d346",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "390b166c02a0c1d81753a9ac3a404fc8",
          "grade": false,
          "grade_id": "tripletas",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "f1AZarC3I_cG",
        "outputId": "e9922c4e-77c6-4f5a-b990-bed86ca5fdd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      Region                               Bus/minibus  \\\n",
            "0               East England  {'col1': 44, 'col2': 4.34, 'col3': 1.77}   \n",
            "1              East Midlands  {'col1': 39, 'col2': 4.95, 'col3': 1.85}   \n",
            "2                     London  {'col1': 28, 'col2': 4.71, 'col3': 1.75}   \n",
            "3         North East England  {'col1': 52, 'col2': 4.46, 'col3': 1.85}   \n",
            "4         North West England   {'col1': 66, 'col2': 4.7, 'col3': 1.88}   \n",
            "5                   Scotland     {'col1': 2, 'col2': 4.0, 'col3': 1.5}   \n",
            "6         South East England  {'col1': 71, 'col2': 4.96, 'col3': 1.87}   \n",
            "7         South West England  {'col1': 37, 'col2': 4.84, 'col3': 1.81}   \n",
            "8                      Wales     {'col1': 1, 'col2': 5.0, 'col3': 2.0}   \n",
            "9              Wast Midlands   {'col1': 56, 'col2': 4.7, 'col3': 1.91}   \n",
            "10  Yorkshire and the Humber  {'col1': 75, 'col2': 4.67, 'col3': 1.69}   \n",
            "\n",
            "                                            Car  \\\n",
            "0    {'col1': 22813, 'col2': 3.95, 'col3': 2.0}   \n",
            "1   {'col1': 17191, 'col2': 3.91, 'col3': 1.96}   \n",
            "2    {'col1': 24015, 'col2': 3.9, 'col3': 1.92}   \n",
            "3    {'col1': 9272, 'col2': 3.96, 'col3': 1.96}   \n",
            "4   {'col1': 25783, 'col2': 4.01, 'col3': 1.97}   \n",
            "5     {'col1': 198, 'col2': 4.24, 'col3': 1.45}   \n",
            "6   {'col1': 39410, 'col2': 4.03, 'col3': 2.02}   \n",
            "7   {'col1': 21144, 'col2': 4.11, 'col3': 1.99}   \n",
            "8     {'col1': 282, 'col2': 4.23, 'col3': 1.73}   \n",
            "9   {'col1': 20933, 'col2': 3.85, 'col3': 1.97}   \n",
            "10  {'col1': 23049, 'col2': 3.93, 'col3': 1.96}   \n",
            "\n",
            "                                    Motorcycle  \\\n",
            "0    {'col1': 2753, 'col2': 3.17, 'col3': 1.8}   \n",
            "1   {'col1': 2084, 'col2': 3.29, 'col3': 1.79}   \n",
            "2   {'col1': 5658, 'col2': 3.22, 'col3': 1.82}   \n",
            "3    {'col1': 742, 'col2': 3.46, 'col3': 1.79}   \n",
            "4   {'col1': 2724, 'col2': 3.29, 'col3': 1.82}   \n",
            "5     {'col1': 65, 'col2': 4.77, 'col3': 1.25}   \n",
            "6    {'col1': 5281, 'col2': 3.28, 'col3': 1.8}   \n",
            "7   {'col1': 2802, 'col2': 3.17, 'col3': 1.88}   \n",
            "8     {'col1': 79, 'col2': 4.35, 'col3': 1.51}   \n",
            "9   {'col1': 2168, 'col2': 3.16, 'col3': 1.88}   \n",
            "10  {'col1': 2537, 'col2': 3.27, 'col3': 1.82}   \n",
            "\n",
            "                                       Other  \\\n",
            "0   {'col1': 61, 'col2': 4.02, 'col3': 1.87}   \n",
            "1   {'col1': 48, 'col2': 4.44, 'col3': 1.92}   \n",
            "2   {'col1': 56, 'col2': 3.89, 'col3': 1.84}   \n",
            "3   {'col1': 32, 'col2': 3.81, 'col3': 2.06}   \n",
            "4   {'col1': 66, 'col2': 4.47, 'col3': 1.98}   \n",
            "5      {'col1': 2, 'col2': 7.0, 'col3': 1.5}   \n",
            "6   {'col1': 145, 'col2': 3.81, 'col3': 2.0}   \n",
            "7    {'col1': 67, 'col2': 4.49, 'col3': 2.0}   \n",
            "8      {'col1': 2, 'col2': 2.5, 'col3': 1.5}   \n",
            "9   {'col1': 52, 'col2': 3.96, 'col3': 1.92}   \n",
            "10  {'col1': 75, 'col2': 3.92, 'col3': 1.99}   \n",
            "\n",
            "                                            Taxi  \\\n",
            "0    {'col1': 424.0, 'col2': 4.43, 'col3': 1.89}   \n",
            "1    {'col1': 362.0, 'col2': 4.36, 'col3': 1.83}   \n",
            "2   {'col1': 1671.0, 'col2': 4.63, 'col3': 1.79}   \n",
            "3    {'col1': 279.0, 'col2': 4.12, 'col3': 1.67}   \n",
            "4   {'col1': 1070.0, 'col2': 4.33, 'col3': 1.75}   \n",
            "5        {'col1': 1.0, 'col2': 5.0, 'col3': 1.0}   \n",
            "6    {'col1': 787.0, 'col2': 4.42, 'col3': 1.78}   \n",
            "7    {'col1': 317.0, 'col2': 4.58, 'col3': 1.74}   \n",
            "8                                           None   \n",
            "9    {'col1': 607.0, 'col2': 4.14, 'col3': 1.79}   \n",
            "10   {'col1': 742.0, 'col2': 4.15, 'col3': 1.75}   \n",
            "\n",
            "                                           Van  \n",
            "0   {'col1': 1470, 'col2': 3.91, 'col3': 2.12}  \n",
            "1   {'col1': 1117, 'col2': 3.98, 'col3': 2.06}  \n",
            "2   {'col1': 2212, 'col2': 3.88, 'col3': 1.91}  \n",
            "3     {'col1': 639, 'col2': 3.95, 'col3': 2.0}  \n",
            "4   {'col1': 1387, 'col2': 3.91, 'col3': 2.01}  \n",
            "5      {'col1': 26, 'col2': 4.0, 'col3': 1.46}  \n",
            "6    {'col1': 2548, 'col2': 3.92, 'col3': 2.1}  \n",
            "7   {'col1': 1236, 'col2': 3.97, 'col3': 2.03}  \n",
            "8     {'col1': 29, 'col2': 3.52, 'col3': 1.83}  \n",
            "9   {'col1': 1436, 'col2': 3.91, 'col3': 2.03}  \n",
            "10  {'col1': 1412, 'col2': 3.92, 'col3': 2.02}  \n"
          ]
        }
      ],
      "source": [
        "# Agrupo el DataFrame original, por Región y Tipo de Vehículo\n",
        "# y calculo tres métricas para cada combinación:\n",
        "# 1. Número total de accidentes.\n",
        "# 2. Edad promedio del conductor (redondeada a 2 decimales).\n",
        "# 3. Promedio de vehículos implicados (redondeado a 2 decimales).\n",
        "# Luego empaqueto estas 3 métricas, en una sola estructura (struct).\n",
        "grupo = accidentesDF.groupBy(\"Region\", \"Vehicle_Category\").agg(\n",
        "    F.struct(\n",
        "        F.count(\"*\"),                               # Total de accidentes.\n",
        "        F.round(F.avg(\"Age_of_Driver\"), 2),         # Edad media del conductor.\n",
        "        F.round(F.avg(\"Number_of_Vehicles\"), 2)     # Promedio de vehículos implicados.\n",
        "    ).alias(\"resumen\")                              # Le doy el nombre \"resumen\" a esta estructura.\n",
        ")\n",
        "\n",
        "# A partir del resultado anterior, hago la agrupación solo por Región\n",
        "# y aplico pivot, para convertir los tipos de vehículos en columnas individuales.\n",
        "# Para cada combinación, selecciono el primer valor del resumen (ya que solo hay uno por grupo).\n",
        "numero_edad_coches_df = (\n",
        "    grupo\n",
        "    .groupBy(\"Region\")           # Agrupo solo por región\n",
        "    .pivot(\"Vehicle_Category\")   # Cada tipo de vehículo, será una columna.\n",
        "    .agg(F.first(\"resumen\"))     # Tomo la estructura calculada, para cada tipo\n",
        "    .orderBy(\"Region\")           # Ordeno alfabéticamente, por Región.\n",
        ")\n",
        "\n",
        "# Convierto el DataFrame de Spark, a un DataFrame de Pandas\n",
        "# para poder visualizarlo más fácilmente, como tabla o exportarlo si fuera necesario.\n",
        "numero_edad_coches_pd = numero_edad_coches_df.toPandas()\n",
        "\n",
        "# Mostro por pantalla el resultado final en formato Pandas.\n",
        "# Cada celda contiene una tupla: (nº de accidentes, edad media, nº vehículos)\n",
        "print(numero_edad_coches_pd)\n",
        "\n",
        "# COMENTARIO GENERAL DEL CÓDIGO:\n",
        "# Este bloque analiza los accidentes por región y tipo de vehículo.\n",
        "# Para cada combinación se calcula el número de accidentes, la edad media del conductor\n",
        "# y el promedio de vehículos implicados. Estos datos se empaquetan en estructuras (structs)\n",
        "# y se pivotan para formar un resumen por región, con columnas separadas por tipo de vehículo.\n",
        "# Finalmente, se convierte a Pandas para facilitar su visualización.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e0e335db-3092-4a46-913e-ac9f0446512c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a55def89481fb11a028a5f098d25f383",
          "grade": true,
          "grade_id": "tripletas_test",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "l1iyPfjqI_cH"
      },
      "outputs": [],
      "source": [
        "assert(len(numero_edad_coches_df.columns) == 7)\n",
        "assert(sum([1 for c in [\"Region\", \"Bus/minibus\", \"Car\", \"Motorcycle\", \"Other\", \"Taxi\", \"Van\"]\n",
        "          if c in numero_edad_coches_df.columns]) == 7)\n",
        "r = numero_edad_coches_df.collect()\n",
        "assert(r[0].Region == \"East England\" and r[0].Other == (61, 4.02, 1.87))\n",
        "assert(len(numero_edad_coches_df.columns) == 7 and len(r) == 11)\n",
        "assert(r[0].Car == (22813, 3.95, 2.0))\n",
        "assert(r[7].Motorcycle == (2802, 3.17, 1.88))\n",
        "assert(r[10].Van == (1412, 3.92, 2.02))"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 6584647907221975,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "Spark_m6",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
